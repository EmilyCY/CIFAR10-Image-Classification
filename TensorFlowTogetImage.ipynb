{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94308695-f8d8-4646-b128-d7a8ac134400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.9.1\n",
      "  Downloading tensorflow-2.9.1-cp39-cp39-macosx_10_14_x86_64.whl (228.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 228.5 MB 18 kB/s s eta 0:00:01   |█▍                              | 10.3 MB 5.7 MB/s eta 0:00:39\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.1.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.34.1)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (0.2.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard<2.10,>=2.9 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.12.1)\n",
      "Requirement already satisfied: packaging in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (21.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.12)\n",
      "Requirement already satisfied: setuptools in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (58.0.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.9.1) (1.6.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-macosx_10_14_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-macosx_10_9_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.6.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/dhvanit/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow==2.9.1) (3.0.4)\n",
      "Installing collected packages: protobuf, numpy, absl-py, tensorflow-io-gcs-filesystem, tensorflow-estimator, libclang, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.1\n",
      "    Uninstalling protobuf-4.21.1:\n",
      "      Successfully uninstalled protobuf-4.21.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.5.0\n",
      "    Uninstalling tensorflow-2.5.0:\n",
      "      Successfully uninstalled tensorflow-2.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\n",
      "bokeh 2.4.1 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 keras-2.9.0 libclang-14.0.1 numpy-1.22.4 protobuf-3.19.4 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586e569-8ac9-4b26-9367-66b0f7bb6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def normalization(train_images, test_images):\n",
    "    mean = np.mean(train_images, axis=(0, 1, 2, 3))\n",
    "    std = np.std(train_images, axis=(0, 1, 2, 3))\n",
    "    train_images = (train_images - mean) / (std + 1e-7)\n",
    "    test_images = (test_images - mean) / (std + 1e-7)\n",
    "    return train_images, test_images\n",
    "\n",
    "\n",
    "def load_images():\n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "    train_images = train_images.astype(np.float32)\n",
    "    test_images = test_images.astype(np.float32)\n",
    "\n",
    "    (train_images, test_images) = normalization(train_images, test_images)\n",
    "\n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "    # train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
    "    #     buffer_size=10000).batch(batch_size)\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "class ConvBNRelu(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, padding='SAME', weight_decay=0.0005, rate=0.4, drop=True):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.drop = drop\n",
    "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                        padding=padding, kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dropOut = keras.layers.Dropout(rate=rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        layer = self.conv(inputs)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = self.batchnorm(layer)\n",
    "        if self.drop:\n",
    "            layer = self.dropOut(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "class VGG16Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Model, self).__init__()\n",
    "        self.conv1 = ConvBNRelu(filters=64, kernel_size=[3, 3], rate=0.3)\n",
    "        self.conv2 = ConvBNRelu(filters=64, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv3 = ConvBNRelu(filters=128, kernel_size=[3, 3])\n",
    "        self.conv4 = ConvBNRelu(filters=128, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv5 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv6 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv7 = ConvBNRelu(filters=256, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling3 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv11 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv12 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv13 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling5 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv14 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv15 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv16 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling6 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        self.dropOut = keras.layers.Dropout(rate=0.5)\n",
    "        self.dense1 = keras.layers.Dense(units=512,\n",
    "                                         activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dense2 = keras.layers.Dense(units=10)\n",
    "        self.softmax = keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.conv2(net)\n",
    "        net = self.maxPooling1(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.conv4(net)\n",
    "        net = self.maxPooling2(net)\n",
    "        net = self.conv5(net)\n",
    "        net = self.conv6(net)\n",
    "        net = self.conv7(net)\n",
    "        net = self.maxPooling3(net)\n",
    "        net = self.conv11(net)\n",
    "        net = self.conv12(net)\n",
    "        net = self.conv13(net)\n",
    "        net = self.maxPooling5(net)\n",
    "        net = self.conv14(net)\n",
    "        net = self.conv15(net)\n",
    "        net = self.conv16(net)\n",
    "        net = self.maxPooling6(net)\n",
    "        net = self.dropOut(net)\n",
    "        net = self.flat(net)\n",
    "        net = self.dense1(net)\n",
    "        net = self.batchnorm(net)\n",
    "        net = self.drop(net)\n",
    "        net = self.dense2(net)\n",
    "        net = self.softmax(net)\n",
    "        return net\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(tf.__version__)\n",
    "    print(keras.__version__)\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "        try:\n",
    "            print('start with GPU 7')\n",
    "            tf.config.experimental.set_visible_devices(gpus[7], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[7], True)\n",
    "        except RuntimeError as e:\n",
    "            # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "            print(e)\n",
    "\n",
    "    training_epochs = 250\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.9\n",
    "    lr_decay = 1e-6\n",
    "    lr_drop = 20\n",
    "\n",
    "    tf.random.set_seed(777)\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "\n",
    "    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = load_images()\n",
    "\n",
    "    # data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    model = VGG16Model()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                        decay=1e-6, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(datagen.flow(train_images, train_labels,\n",
    "                                     batch_size=batch_size), epochs=training_epochs, verbose=2, callbacks=[reduce_lr],\n",
    "                        steps_per_epoch=train_images.shape[0] // batch_size, validation_data=(test_images, test_labels))\n",
    "\n",
    "    model.save_weights('cifar10vgg_custom.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54d416-cd0b-4aa7-83d1-4ca23ec987c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
